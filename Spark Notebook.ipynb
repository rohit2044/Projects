{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my first spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkContext in module pyspark.context object:\n",
      "\n",
      "class SparkContext(builtins.object)\n",
      " |  SparkContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
      " |  \n",
      " |  Main entry point for Spark functionality. A SparkContext represents the\n",
      " |  connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
      " |  broadcast variables on that cluster.\n",
      " |  \n",
      " |  .. note:: Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
      " |      the active :class:`SparkContext` before creating a new one.\n",
      " |  \n",
      " |  .. note:: :class:`SparkContext` instance is not supported to share across multiple\n",
      " |      processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
      " |      Use threads instead for concurrent processing purpose.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
      " |  \n",
      " |  __exit__(self, type, value, trace)\n",
      " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the context on exit of the with block.\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
      " |      Create a new SparkContext. At least the master and app name should be set,\n",
      " |      either through the named parameters here or through `conf`.\n",
      " |      \n",
      " |      :param master: Cluster URL to connect to\n",
      " |             (e.g. mesos://host:port, spark://host:port, local[4]).\n",
      " |      :param appName: A name for your job, to display on the cluster web UI.\n",
      " |      :param sparkHome: Location where Spark is installed on cluster nodes.\n",
      " |      :param pyFiles: Collection of .zip or .py files to send to the cluster\n",
      " |             and add to PYTHONPATH.  These can be paths on the local file\n",
      " |             system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
      " |      :param environment: A dictionary of environment variables to set on\n",
      " |             worker nodes.\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. Set 1 to disable batching, 0 to automatically choose\n",
      " |             the batch size based on object sizes, or -1 to use an unlimited\n",
      " |             batch size\n",
      " |      :param serializer: The serializer for RDDs.\n",
      " |      :param conf: A :class:`SparkConf` object setting Spark properties.\n",
      " |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n",
      " |             will be instantiated.\n",
      " |      :param jsc: The JavaSparkContext instance (optional).\n",
      " |      :param profiler_cls: A class of custom Profiler used to do profiling\n",
      " |             (default is pyspark.profiler.BasicProfiler).\n",
      " |      \n",
      " |      \n",
      " |      >>> from pyspark.context import SparkContext\n",
      " |      >>> sc = SparkContext('local', 'test')\n",
      " |      \n",
      " |      >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError:...\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  accumulator(self, value, accum_param=None)\n",
      " |      Create an :class:`Accumulator` with the given initial value, using a given\n",
      " |      :class:`AccumulatorParam` helper object to define how to add values of the\n",
      " |      data type if provided. Default AccumulatorParams are used for integers\n",
      " |      and floating-point numbers if you do not provide one. For other types,\n",
      " |      a custom AccumulatorParam can be used.\n",
      " |  \n",
      " |  addFile(self, path, recursive=False)\n",
      " |      Add a file to be downloaded with this Spark job on every node.\n",
      " |      The `path` passed can be either a local file, a file in HDFS\n",
      " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      " |      FTP URI.\n",
      " |      \n",
      " |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n",
      " |      filename to find its download location.\n",
      " |      \n",
      " |      A directory can be given if the recursive option is set to True.\n",
      " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
      " |      \n",
      " |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n",
      " |      \n",
      " |      >>> from pyspark import SparkFiles\n",
      " |      >>> path = os.path.join(tempdir, \"test.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"100\")\n",
      " |      >>> sc.addFile(path)\n",
      " |      >>> def func(iterator):\n",
      " |      ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
      " |      ...        fileVal = int(testFile.readline())\n",
      " |      ...        return [x * fileVal for x in iterator]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      " |      [100, 200, 300, 400]\n",
      " |  \n",
      " |  addPyFile(self, path)\n",
      " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
      " |      SparkContext in the future.  The `path` passed can be either a local\n",
      " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
      " |      HTTP, HTTPS or FTP URI.\n",
      " |      \n",
      " |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n",
      " |  \n",
      " |  binaryFiles(self, path, minPartitions=None)\n",
      " |      Read a directory of binary files from HDFS, a local file system\n",
      " |      (available on all nodes), or any Hadoop-supported file system URI\n",
      " |      as a byte array. Each file is read as a single record and returned\n",
      " |      in a key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      .. note:: Small files are preferred, large file is also allowable, but\n",
      " |          may cause bad performance.\n",
      " |  \n",
      " |  binaryRecords(self, path, recordLength)\n",
      " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
      " |      with the specified numerical format (see ByteBuffer), and the number of\n",
      " |      bytes per record is constant.\n",
      " |      \n",
      " |      :param path: Directory to the input data files\n",
      " |      :param recordLength: The length at which to split the records\n",
      " |  \n",
      " |  broadcast(self, value)\n",
      " |      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n",
      " |      object for reading it in distributed functions. The variable will\n",
      " |      be sent to each cluster only once.\n",
      " |  \n",
      " |  cancelAllJobs(self)\n",
      " |      Cancel all jobs that have been scheduled or are running.\n",
      " |  \n",
      " |  cancelJobGroup(self, groupId)\n",
      " |      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n",
      " |      for more information.\n",
      " |  \n",
      " |  dump_profiles(self, path)\n",
      " |      Dump the profile stats into directory `path`\n",
      " |  \n",
      " |  emptyRDD(self)\n",
      " |      Create an RDD that has no partitions or elements.\n",
      " |  \n",
      " |  getConf(self)\n",
      " |  \n",
      " |  getLocalProperty(self, key)\n",
      " |      Get a local property set in this thread, or null if it is missing. See\n",
      " |      :meth:`setLocalProperty`.\n",
      " |  \n",
      " |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  parallelize(self, c, numSlices=None)\n",
      " |      Distribute a local Python collection to form an RDD. Using xrange\n",
      " |      is recommended if the input represents a range for performance.\n",
      " |      \n",
      " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      " |      [[0], [2], [3], [4], [6]]\n",
      " |      >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      " |      [[], [0], [], [2], [4]]\n",
      " |  \n",
      " |  pickleFile(self, name, minPartitions=None)\n",
      " |      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numSlices=None)\n",
      " |      Create a new RDD of int containing elements from `start` to `end`\n",
      " |      (exclusive), increased by `step` every element. Can be called the same\n",
      " |      way as python's built-in range() function. If called with a single argument,\n",
      " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numSlices: the number of partitions of the new RDD\n",
      " |      :return: An RDD of int\n",
      " |      \n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.range(2, 4).collect()\n",
      " |      [2, 3]\n",
      " |      >>> sc.range(1, 7, 2).collect()\n",
      " |      [1, 3, 5]\n",
      " |  \n",
      " |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n",
      " |      Executes the given partitionFunc on the specified set of partitions,\n",
      " |      returning the result as an array of elements.\n",
      " |      \n",
      " |      If 'partitions' is not specified, this will run over all partitions.\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
      " |      [0, 1, 4, 9, 16, 25]\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
      " |      [0, 1, 16, 25]\n",
      " |  \n",
      " |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n",
      " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is as follows:\n",
      " |      \n",
      " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
      " |             and value Writable classes\n",
      " |          2. Serialization is attempted via Pyrolite pickling\n",
      " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
      " |          4. :class:`PickleSerializer` is used to deserialize pickled objects on the Python side\n",
      " |      \n",
      " |      :param path: path to sequncefile\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter:\n",
      " |      :param valueConverter:\n",
      " |      :param minSplits: minimum splits in dataset\n",
      " |             (default min(2, sc.defaultParallelism))\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  setCheckpointDir(self, dirName)\n",
      " |      Set the directory under which RDDs are going to be checkpointed. The\n",
      " |      directory must be an HDFS path if running on a cluster.\n",
      " |  \n",
      " |  setJobDescription(self, value)\n",
      " |      Set a human readable description of the current job.\n",
      " |      \n",
      " |      .. note:: Currently, setting a job description (set to local properties) with multiple\n",
      " |          threads does not properly work. Internally threads on PVM and JVM are not synced,\n",
      " |          and JVM thread can be reused for multiple threads on PVM, which fails to isolate\n",
      " |          local properties for each thread on PVM.\n",
      " |      \n",
      " |          To work around this, you can set `PYSPARK_PIN_THREAD` to\n",
      " |          `'true'` (see SPARK-22340). However, note that it cannot inherit the local properties\n",
      " |          from the parent thread although it isolates each thread on PVM and JVM with its own\n",
      " |          local properties.\n",
      " |      \n",
      " |          To work around this, you should manually copy and set the local\n",
      " |          properties from the parent thread to the child thread when you create another thread.\n",
      " |  \n",
      " |  setJobGroup(self, groupId, description, interruptOnCancel=False)\n",
      " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
      " |      different value or cleared.\n",
      " |      \n",
      " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
      " |      Application programmers can use this method to group all those jobs together and give a\n",
      " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
      " |      \n",
      " |      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n",
      " |      running jobs in this group.\n",
      " |      \n",
      " |      >>> import threading\n",
      " |      >>> from time import sleep\n",
      " |      >>> result = \"Not Set\"\n",
      " |      >>> lock = threading.Lock()\n",
      " |      >>> def map_func(x):\n",
      " |      ...     sleep(100)\n",
      " |      ...     raise Exception(\"Task should have been cancelled\")\n",
      " |      >>> def start_job(x):\n",
      " |      ...     global result\n",
      " |      ...     try:\n",
      " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
      " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      " |      ...     except Exception as e:\n",
      " |      ...         result = \"Cancelled\"\n",
      " |      ...     lock.release()\n",
      " |      >>> def stop_job():\n",
      " |      ...     sleep(5)\n",
      " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
      " |      >>> suppress = lock.acquire()\n",
      " |      >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n",
      " |      >>> suppress = threading.Thread(target=stop_job).start()\n",
      " |      >>> suppress = lock.acquire()\n",
      " |      >>> print(result)\n",
      " |      Cancelled\n",
      " |      \n",
      " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
      " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
      " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
      " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
      " |      \n",
      " |      .. note:: Currently, setting a group ID (set to local properties) with multiple threads\n",
      " |          does not properly work. Internally threads on PVM and JVM are not synced, and JVM\n",
      " |          thread can be reused for multiple threads on PVM, which fails to isolate local\n",
      " |          properties for each thread on PVM.\n",
      " |      \n",
      " |          To work around this, you can set `PYSPARK_PIN_THREAD` to\n",
      " |          `'true'` (see SPARK-22340). However, note that it cannot inherit the local properties\n",
      " |          from the parent thread although it isolates each thread on PVM and JVM with its own\n",
      " |          local properties.\n",
      " |      \n",
      " |          To work around this, you should manually copy and set the local\n",
      " |          properties from the parent thread to the child thread when you create another thread.\n",
      " |  \n",
      " |  setLocalProperty(self, key, value)\n",
      " |      Set a local property that affects jobs submitted from this thread, such as the\n",
      " |      Spark fair scheduler pool.\n",
      " |      \n",
      " |      .. note:: Currently, setting a local property with multiple threads does not properly work.\n",
      " |          Internally threads on PVM and JVM are not synced, and JVM thread\n",
      " |          can be reused for multiple threads on PVM, which fails to isolate local properties\n",
      " |          for each thread on PVM.\n",
      " |      \n",
      " |          To work around this, you can set `PYSPARK_PIN_THREAD` to\n",
      " |          `'true'` (see SPARK-22340). However, note that it cannot inherit the local properties\n",
      " |          from the parent thread although it isolates each thread on PVM and JVM with its own\n",
      " |          local properties.\n",
      " |      \n",
      " |          To work around this, you should manually copy and set the local\n",
      " |          properties from the parent thread to the child thread when you create another thread.\n",
      " |  \n",
      " |  setLogLevel(self, logLevel)\n",
      " |      Control our logLevel. This overrides any user-defined log settings.\n",
      " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
      " |  \n",
      " |  show_profiles(self)\n",
      " |      Print the profile stats to stdout\n",
      " |  \n",
      " |  sparkUser(self)\n",
      " |      Get SPARK_USER for user who is running SparkContext.\n",
      " |  \n",
      " |  statusTracker(self)\n",
      " |      Return :class:`StatusTracker` object\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Shut down the SparkContext.\n",
      " |  \n",
      " |  textFile(self, name, minPartitions=None, use_unicode=True)\n",
      " |      Read a text file from HDFS, a local file system (available on all\n",
      " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
      " |      RDD of Strings.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello world!\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      ['Hello world!']\n",
      " |  \n",
      " |  union(self, rdds)\n",
      " |      Build the union of a list of RDDs.\n",
      " |      \n",
      " |      This supports unions() of RDDs with different serialized formats,\n",
      " |      although this forces them to be reserialized using the default\n",
      " |      serializer:\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"union-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      ['Hello']\n",
      " |      >>> parallelized = sc.parallelize([\"World!\"])\n",
      " |      >>> sorted(sc.union([textFile, parallelized]).collect())\n",
      " |      ['Hello', 'World!']\n",
      " |  \n",
      " |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n",
      " |      Read a directory of text files from HDFS, a local file system\n",
      " |      (available on all nodes), or any  Hadoop-supported file system\n",
      " |      URI. Each file is read as a single record and returned in a\n",
      " |      key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      For example, if you have the following files:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          hdfs://a-hdfs-path/part-00000\n",
      " |          hdfs://a-hdfs-path/part-00001\n",
      " |          ...\n",
      " |          hdfs://a-hdfs-path/part-nnnnn\n",
      " |      \n",
      " |      Do ``rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")``,\n",
      " |      then ``rdd`` contains:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          (a-hdfs-path/part-00000, its content)\n",
      " |          (a-hdfs-path/part-00001, its content)\n",
      " |          ...\n",
      " |          (a-hdfs-path/part-nnnnn, its content)\n",
      " |      \n",
      " |      .. note:: Small files are preferred, as each file will be loaded\n",
      " |          fully in memory.\n",
      " |      \n",
      " |      >>> dirPath = os.path.join(tempdir, \"files\")\n",
      " |      >>> os.mkdir(dirPath)\n",
      " |      >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
      " |      ...    _ = file1.write(\"1\")\n",
      " |      >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
      " |      ...    _ = file2.write(\"2\")\n",
      " |      >>> textFiles = sc.wholeTextFiles(dirPath)\n",
      " |      >>> sorted(textFiles.collect())\n",
      " |      [('.../1.txt', '1'), ('.../2.txt', '2')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(conf=None) from builtins.type\n",
      " |      Get or instantiate a SparkContext and register it as a singleton object.\n",
      " |      \n",
      " |      :param conf: SparkConf (optional)\n",
      " |  \n",
      " |  setSystemProperty(key, value) from builtins.type\n",
      " |      Set a Java system property, such as spark.executor.memory. This must\n",
      " |      must be invoked before instantiating SparkContext.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  applicationId\n",
      " |      A unique identifier for the Spark application.\n",
      " |      Its format depends on the scheduler implementation.\n",
      " |      \n",
      " |      * in case of local spark app something like 'local-1433865536131'\n",
      " |      * in case of YARN something like 'application_1433865536131_34483'\n",
      " |      \n",
      " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
      " |      'local-...'\n",
      " |  \n",
      " |  defaultMinPartitions\n",
      " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
      " |  \n",
      " |  defaultParallelism\n",
      " |      Default level of parallelism to use when not given by user (e.g. for\n",
      " |      reduce tasks)\n",
      " |  \n",
      " |  resources\n",
      " |  \n",
      " |  startTime\n",
      " |      Return the epoch time when the Spark Context was started.\n",
      " |  \n",
      " |  uiWebUrl\n",
      " |      Return the URL of the SparkUI instance started by this SparkContext\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'resources',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[1, 2, 3, 4]\n",
    "distributedData=sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,AdjClose,Volume',\n",
       " '2019-07-15,248.000000,254.419998,244.860001,253.500000,253.500000,11000100',\n",
       " '2019-07-16,249.300003,253.529999,247.929993,252.380005,252.380005,8149000',\n",
       " '2019-07-17,255.669998,258.309998,253.350006,254.860001,254.860001,9764700',\n",
       " '2019-07-18,255.050003,255.750000,251.889999,253.539993,253.539993,4764500']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla_file = 'TSLA.csv'\n",
    "tesla_rdd = sc.textFile(tesla_file)\n",
    "tesla_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_rdd = tesla_rdd.map(lambda row:row.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Date', 'Open', 'High', 'Low', 'Close', 'AdjClose', 'Volume'],\n",
       " ['2019-07-15',\n",
       "  '248.000000',\n",
       "  '254.419998',\n",
       "  '244.860001',\n",
       "  '253.500000',\n",
       "  '253.500000',\n",
       "  '11000100'],\n",
       " ['2019-07-16',\n",
       "  '249.300003',\n",
       "  '253.529999',\n",
       "  '247.929993',\n",
       "  '252.380005',\n",
       "  '252.380005',\n",
       "  '8149000'],\n",
       " ['2019-07-17',\n",
       "  '255.669998',\n",
       "  '258.309998',\n",
       "  '253.350006',\n",
       "  '254.860001',\n",
       "  '254.860001',\n",
       "  '9764700'],\n",
       " ['2019-07-18',\n",
       "  '255.050003',\n",
       "  '255.750000',\n",
       "  '251.889999',\n",
       "  '253.539993',\n",
       "  '253.539993',\n",
       "  '4764500'],\n",
       " ['2019-07-19',\n",
       "  '255.690002',\n",
       "  '259.959991',\n",
       "  '254.619995',\n",
       "  '258.179993',\n",
       "  '258.179993',\n",
       "  '7048400'],\n",
       " ['2019-07-22',\n",
       "  '258.750000',\n",
       "  '262.149994',\n",
       "  '254.190002',\n",
       "  '255.679993',\n",
       "  '255.679993',\n",
       "  '6842400'],\n",
       " ['2019-07-23',\n",
       "  '256.709991',\n",
       "  '260.480011',\n",
       "  '254.500000',\n",
       "  '260.170013',\n",
       "  '260.170013',\n",
       "  '5023100'],\n",
       " ['2019-07-24',\n",
       "  '259.170013',\n",
       "  '266.070007',\n",
       "  '258.160004',\n",
       "  '264.880005',\n",
       "  '264.880005',\n",
       "  '11072800'],\n",
       " ['2019-07-25',\n",
       "  '233.500000',\n",
       "  '234.500000',\n",
       "  '225.550003',\n",
       "  '228.820007',\n",
       "  '228.820007',\n",
       "  '22418300'],\n",
       " ['2019-07-26',\n",
       "  '226.919998',\n",
       "  '230.259995',\n",
       "  '222.250000',\n",
       "  '228.039993',\n",
       "  '228.039993',\n",
       "  '10027700'],\n",
       " ['2019-07-29',\n",
       "  '227.089996',\n",
       "  '235.940002',\n",
       "  '226.029999',\n",
       "  '235.770004',\n",
       "  '235.770004',\n",
       "  '9273300'],\n",
       " ['2019-07-30',\n",
       "  '232.899994',\n",
       "  '243.360001',\n",
       "  '232.179993',\n",
       "  '242.259995',\n",
       "  '242.259995',\n",
       "  '8109000'],\n",
       " ['2019-07-31',\n",
       "  '243.000000',\n",
       "  '246.679993',\n",
       "  '236.649994',\n",
       "  '241.610001',\n",
       "  '241.610001',\n",
       "  '9178200'],\n",
       " ['2019-08-01',\n",
       "  '242.649994',\n",
       "  '244.509995',\n",
       "  '231.770004',\n",
       "  '233.850006',\n",
       "  '233.850006',\n",
       "  '8259500'],\n",
       " ['2019-08-02',\n",
       "  '231.350006',\n",
       "  '236.270004',\n",
       "  '229.229996',\n",
       "  '234.339996',\n",
       "  '234.339996',\n",
       "  '6136500'],\n",
       " ['2019-08-05',\n",
       "  '229.600006',\n",
       "  '231.369995',\n",
       "  '225.779999',\n",
       "  '228.320007',\n",
       "  '228.320007',\n",
       "  '7028300'],\n",
       " ['2019-08-06',\n",
       "  '231.880005',\n",
       "  '232.500000',\n",
       "  '225.750000',\n",
       "  '230.750000',\n",
       "  '230.750000',\n",
       "  '5564200'],\n",
       " ['2019-08-07',\n",
       "  '226.500000',\n",
       "  '233.570007',\n",
       "  '225.800003',\n",
       "  '233.419998',\n",
       "  '233.419998',\n",
       "  '4776500'],\n",
       " ['2019-08-08',\n",
       "  '234.449997',\n",
       "  '239.800003',\n",
       "  '232.649994',\n",
       "  '238.300003',\n",
       "  '238.300003',\n",
       "  '5274300'],\n",
       " ['2019-08-09',\n",
       "  '236.050003',\n",
       "  '238.960007',\n",
       "  '233.809998',\n",
       "  '235.009995',\n",
       "  '235.009995',\n",
       "  '3898200'],\n",
       " ['2019-08-12',\n",
       "  '232.990005',\n",
       "  '235.770004',\n",
       "  '228.750000',\n",
       "  '229.009995',\n",
       "  '229.009995',\n",
       "  '4663900'],\n",
       " ['2019-08-13',\n",
       "  '228.809998',\n",
       "  '236.000000',\n",
       "  '227.550003',\n",
       "  '235.000000',\n",
       "  '235.000000',\n",
       "  '4848100'],\n",
       " ['2019-08-14',\n",
       "  '231.210007',\n",
       "  '231.500000',\n",
       "  '216.690002',\n",
       "  '219.619995',\n",
       "  '219.619995',\n",
       "  '9562600'],\n",
       " ['2019-08-15',\n",
       "  '220.860001',\n",
       "  '221.559998',\n",
       "  '211.550003',\n",
       "  '215.639999',\n",
       "  '215.639999',\n",
       "  '8159600'],\n",
       " ['2019-08-16',\n",
       "  '216.660004',\n",
       "  '222.240005',\n",
       "  '216.020004',\n",
       "  '219.940002',\n",
       "  '219.940002',\n",
       "  '5098500'],\n",
       " ['2019-08-19',\n",
       "  '224.210007',\n",
       "  '227.830002',\n",
       "  '221.699997',\n",
       "  '226.830002',\n",
       "  '226.830002',\n",
       "  '5309600'],\n",
       " ['2019-08-20',\n",
       "  '227.619995',\n",
       "  '229.089996',\n",
       "  '224.539993',\n",
       "  '225.860001',\n",
       "  '225.860001',\n",
       "  '4125200'],\n",
       " ['2019-08-21',\n",
       "  '222.009995',\n",
       "  '223.220001',\n",
       "  '217.600006',\n",
       "  '220.830002',\n",
       "  '220.830002',\n",
       "  '7794300'],\n",
       " ['2019-08-22',\n",
       "  '222.800003',\n",
       "  '225.399994',\n",
       "  '218.220001',\n",
       "  '222.149994',\n",
       "  '222.149994',\n",
       "  '6559000'],\n",
       " ['2019-08-23',\n",
       "  '219.970001',\n",
       "  '221.169998',\n",
       "  '211.000000',\n",
       "  '211.399994',\n",
       "  '211.399994',\n",
       "  '8538600'],\n",
       " ['2019-08-26',\n",
       "  '213.600006',\n",
       "  '215.020004',\n",
       "  '211.539993',\n",
       "  '215.000000',\n",
       "  '215.000000',\n",
       "  '5051900'],\n",
       " ['2019-08-27',\n",
       "  '215.740005',\n",
       "  '218.800003',\n",
       "  '212.029999',\n",
       "  '214.080002',\n",
       "  '214.080002',\n",
       "  '5416200'],\n",
       " ['2019-08-28',\n",
       "  '213.690002',\n",
       "  '217.250000',\n",
       "  '212.309998',\n",
       "  '215.589996',\n",
       "  '215.589996',\n",
       "  '3225500'],\n",
       " ['2019-08-29',\n",
       "  '219.000000',\n",
       "  '223.399994',\n",
       "  '218.000000',\n",
       "  '221.710007',\n",
       "  '221.710007',\n",
       "  '5179500'],\n",
       " ['2019-08-30',\n",
       "  '229.149994',\n",
       "  '232.440002',\n",
       "  '224.210007',\n",
       "  '225.610001',\n",
       "  '225.610001',\n",
       "  '9320600'],\n",
       " ['2019-09-03',\n",
       "  '224.080002',\n",
       "  '228.949997',\n",
       "  '223.160004',\n",
       "  '225.009995',\n",
       "  '225.009995',\n",
       "  '5354100'],\n",
       " ['2019-09-04',\n",
       "  '226.889999',\n",
       "  '228.460007',\n",
       "  '219.210007',\n",
       "  '220.679993',\n",
       "  '220.679993',\n",
       "  '5761000'],\n",
       " ['2019-09-05',\n",
       "  '222.500000',\n",
       "  '229.800003',\n",
       "  '220.850006',\n",
       "  '229.580002',\n",
       "  '229.580002',\n",
       "  '7395300'],\n",
       " ['2019-09-06',\n",
       "  '227.199997',\n",
       "  '229.639999',\n",
       "  '225.169998',\n",
       "  '227.449997',\n",
       "  '227.449997',\n",
       "  '4189400'],\n",
       " ['2019-09-09',\n",
       "  '230.000000',\n",
       "  '233.759995',\n",
       "  '229.229996',\n",
       "  '231.789993',\n",
       "  '231.789993',\n",
       "  '4802700'],\n",
       " ['2019-09-10',\n",
       "  '230.800003',\n",
       "  '235.539993',\n",
       "  '228.940002',\n",
       "  '235.539993',\n",
       "  '235.539993',\n",
       "  '4883700'],\n",
       " ['2019-09-11',\n",
       "  '237.380005',\n",
       "  '248.169998',\n",
       "  '236.000000',\n",
       "  '247.100006',\n",
       "  '247.100006',\n",
       "  '10042800'],\n",
       " ['2019-09-12',\n",
       "  '247.699997',\n",
       "  '253.500000',\n",
       "  '244.399994',\n",
       "  '245.869995',\n",
       "  '245.869995',\n",
       "  '8581200'],\n",
       " ['2019-09-13',\n",
       "  '246.960007',\n",
       "  '248.449997',\n",
       "  '244.869995',\n",
       "  '245.199997',\n",
       "  '245.199997',\n",
       "  '5313100'],\n",
       " ['2019-09-16',\n",
       "  '246.000000',\n",
       "  '247.429993',\n",
       "  '241.169998',\n",
       "  '242.809998',\n",
       "  '242.809998',\n",
       "  '4728100'],\n",
       " ['2019-09-17',\n",
       "  '242.470001',\n",
       "  '245.600006',\n",
       "  '240.369995',\n",
       "  '244.789993',\n",
       "  '244.789993',\n",
       "  '3865400'],\n",
       " ['2019-09-18',\n",
       "  '245.000000',\n",
       "  '248.169998',\n",
       "  '242.369995',\n",
       "  '243.490005',\n",
       "  '243.490005',\n",
       "  '4170200'],\n",
       " ['2019-09-19',\n",
       "  '246.000000',\n",
       "  '247.940002',\n",
       "  '244.839996',\n",
       "  '246.600006',\n",
       "  '246.600006',\n",
       "  '4795800'],\n",
       " ['2019-09-20',\n",
       "  '246.490005',\n",
       "  '246.949997',\n",
       "  '238.160004',\n",
       "  '240.619995',\n",
       "  '240.619995',\n",
       "  '6353000'],\n",
       " ['2019-09-23',\n",
       "  '240.000000',\n",
       "  '245.179993',\n",
       "  '239.220001',\n",
       "  '241.229996',\n",
       "  '241.229996',\n",
       "  '4340200'],\n",
       " ['2019-09-24',\n",
       "  '241.520004',\n",
       "  '241.990005',\n",
       "  '222.610001',\n",
       "  '223.210007',\n",
       "  '223.210007',\n",
       "  '12891500'],\n",
       " ['2019-09-25',\n",
       "  '224.559998',\n",
       "  '228.979996',\n",
       "  '218.360001',\n",
       "  '228.699997',\n",
       "  '228.699997',\n",
       "  '9427100'],\n",
       " ['2019-09-26',\n",
       "  '230.660004',\n",
       "  '243.309998',\n",
       "  '227.399994',\n",
       "  '242.559998',\n",
       "  '242.559998',\n",
       "  '11884500'],\n",
       " ['2019-09-27',\n",
       "  '242.199997',\n",
       "  '248.710007',\n",
       "  '238.729996',\n",
       "  '242.130005',\n",
       "  '242.130005',\n",
       "  '11116400'],\n",
       " ['2019-09-30',\n",
       "  '243.000000',\n",
       "  '243.979996',\n",
       "  '236.110001',\n",
       "  '240.869995',\n",
       "  '240.869995',\n",
       "  '5879800'],\n",
       " ['2019-10-01',\n",
       "  '241.500000',\n",
       "  '245.949997',\n",
       "  '239.130005',\n",
       "  '244.690002',\n",
       "  '244.690002',\n",
       "  '6162600'],\n",
       " ['2019-10-02',\n",
       "  '243.289993',\n",
       "  '244.649994',\n",
       "  '239.429993',\n",
       "  '243.130005',\n",
       "  '243.130005',\n",
       "  '5631400'],\n",
       " ['2019-10-03',\n",
       "  '231.860001',\n",
       "  '234.479996',\n",
       "  '224.279999',\n",
       "  '233.029999',\n",
       "  '233.029999',\n",
       "  '15084500'],\n",
       " ['2019-10-04',\n",
       "  '231.610001',\n",
       "  '234.779999',\n",
       "  '228.070007',\n",
       "  '231.429993',\n",
       "  '231.429993',\n",
       "  '7995000'],\n",
       " ['2019-10-07',\n",
       "  '229.800003',\n",
       "  '238.559998',\n",
       "  '228.550003',\n",
       "  '237.720001',\n",
       "  '237.720001',\n",
       "  '8064200'],\n",
       " ['2019-10-08',\n",
       "  '235.869995',\n",
       "  '243.940002',\n",
       "  '234.500000',\n",
       "  '240.050003',\n",
       "  '240.050003',\n",
       "  '8678200'],\n",
       " ['2019-10-09',\n",
       "  '241.320007',\n",
       "  '247.300003',\n",
       "  '240.649994',\n",
       "  '244.529999',\n",
       "  '244.529999',\n",
       "  '6894400'],\n",
       " ['2019-10-10',\n",
       "  '245.279999',\n",
       "  '249.279999',\n",
       "  '241.580002',\n",
       "  '244.740005',\n",
       "  '244.740005',\n",
       "  '6283300'],\n",
       " ['2019-10-11',\n",
       "  '247.149994',\n",
       "  '251.080002',\n",
       "  '246.809998',\n",
       "  '247.889999',\n",
       "  '247.889999',\n",
       "  '8475400'],\n",
       " ['2019-10-14',\n",
       "  '247.899994',\n",
       "  '258.549988',\n",
       "  '247.130005',\n",
       "  '256.959991',\n",
       "  '256.959991',\n",
       "  '10205000'],\n",
       " ['2019-10-15',\n",
       "  '257.700012',\n",
       "  '260.000000',\n",
       "  '254.119995',\n",
       "  '257.890015',\n",
       "  '257.890015',\n",
       "  '6432800'],\n",
       " ['2019-10-16',\n",
       "  '257.390015',\n",
       "  '262.100006',\n",
       "  '256.920013',\n",
       "  '259.750000',\n",
       "  '259.750000',\n",
       "  '6684100'],\n",
       " ['2019-10-17',\n",
       "  '262.500000',\n",
       "  '264.779999',\n",
       "  '260.170013',\n",
       "  '261.970001',\n",
       "  '261.970001',\n",
       "  '4769300'],\n",
       " ['2019-10-18',\n",
       "  '260.700012',\n",
       "  '262.799988',\n",
       "  '255.100006',\n",
       "  '256.950012',\n",
       "  '256.950012',\n",
       "  '5749800'],\n",
       " ['2019-10-21',\n",
       "  '258.329987',\n",
       "  '259.500000',\n",
       "  '250.179993',\n",
       "  '253.500000',\n",
       "  '253.500000',\n",
       "  '5020300'],\n",
       " ['2019-10-22',\n",
       "  '254.320007',\n",
       "  '258.329987',\n",
       "  '250.850006',\n",
       "  '255.580002',\n",
       "  '255.580002',\n",
       "  '4600800'],\n",
       " ['2019-10-23',\n",
       "  '254.500000',\n",
       "  '256.140015',\n",
       "  '251.350006',\n",
       "  '254.679993',\n",
       "  '254.679993',\n",
       "  '5261100'],\n",
       " ['2019-10-24',\n",
       "  '298.369995',\n",
       "  '304.929993',\n",
       "  '289.200012',\n",
       "  '299.679993',\n",
       "  '299.679993',\n",
       "  '29720900'],\n",
       " ['2019-10-25',\n",
       "  '297.720001',\n",
       "  '330.000000',\n",
       "  '296.109985',\n",
       "  '328.130005',\n",
       "  '328.130005',\n",
       "  '30006100'],\n",
       " ['2019-10-28',\n",
       "  '327.540009',\n",
       "  '340.839996',\n",
       "  '322.600006',\n",
       "  '327.709991',\n",
       "  '327.709991',\n",
       "  '18870300'],\n",
       " ['2019-10-29',\n",
       "  '319.989990',\n",
       "  '324.299988',\n",
       "  '314.750000',\n",
       "  '316.220001',\n",
       "  '316.220001',\n",
       "  '12684300'],\n",
       " ['2019-10-30',\n",
       "  '313.000000',\n",
       "  '318.790009',\n",
       "  '309.970001',\n",
       "  '315.010010',\n",
       "  '315.010010',\n",
       "  '9641800'],\n",
       " ['2019-10-31',\n",
       "  '313.100006',\n",
       "  '319.000000',\n",
       "  '313.000000',\n",
       "  '314.920013',\n",
       "  '314.920013',\n",
       "  '5067000'],\n",
       " ['2019-11-01',\n",
       "  '316.320007',\n",
       "  '316.480011',\n",
       "  '309.799988',\n",
       "  '313.309998',\n",
       "  '313.309998',\n",
       "  '6383900'],\n",
       " ['2019-11-04',\n",
       "  '314.799988',\n",
       "  '321.940002',\n",
       "  '309.260010',\n",
       "  '317.470001',\n",
       "  '317.470001',\n",
       "  '8787000'],\n",
       " ['2019-11-05',\n",
       "  '319.619995',\n",
       "  '323.510010',\n",
       "  '316.119995',\n",
       "  '317.220001',\n",
       "  '317.220001',\n",
       "  '6943400'],\n",
       " ['2019-11-06',\n",
       "  '318.000000',\n",
       "  '326.720001',\n",
       "  '314.500000',\n",
       "  '326.579987',\n",
       "  '326.579987',\n",
       "  '7940900'],\n",
       " ['2019-11-07',\n",
       "  '329.140015',\n",
       "  '341.500000',\n",
       "  '328.019989',\n",
       "  '335.540009',\n",
       "  '335.540009',\n",
       "  '14467300'],\n",
       " ['2019-11-08',\n",
       "  '334.500000',\n",
       "  '337.459991',\n",
       "  '332.500000',\n",
       "  '337.140015',\n",
       "  '337.140015',\n",
       "  '6069200'],\n",
       " ['2019-11-11',\n",
       "  '343.950012',\n",
       "  '349.190002',\n",
       "  '342.000000',\n",
       "  '345.089996',\n",
       "  '345.089996',\n",
       "  '9986700'],\n",
       " ['2019-11-12',\n",
       "  '346.899994',\n",
       "  '350.369995',\n",
       "  '344.040009',\n",
       "  '349.929993',\n",
       "  '349.929993',\n",
       "  '7359400'],\n",
       " ['2019-11-13',\n",
       "  '355.000000',\n",
       "  '356.329987',\n",
       "  '345.179993',\n",
       "  '346.109985',\n",
       "  '346.109985',\n",
       "  '8420100'],\n",
       " ['2019-11-14',\n",
       "  '346.109985',\n",
       "  '353.839996',\n",
       "  '342.910004',\n",
       "  '349.350006',\n",
       "  '349.350006',\n",
       "  '6464900'],\n",
       " ['2019-11-15',\n",
       "  '350.640015',\n",
       "  '352.799988',\n",
       "  '348.359985',\n",
       "  '352.170013',\n",
       "  '352.170013',\n",
       "  '4809000'],\n",
       " ['2019-11-18',\n",
       "  '352.920013',\n",
       "  '353.149994',\n",
       "  '346.100006',\n",
       "  '349.989990',\n",
       "  '349.989990',\n",
       "  '4400400'],\n",
       " ['2019-11-19',\n",
       "  '351.750000',\n",
       "  '359.989990',\n",
       "  '347.799988',\n",
       "  '359.519989',\n",
       "  '359.519989',\n",
       "  '7724800'],\n",
       " ['2019-11-20',\n",
       "  '360.000000',\n",
       "  '361.200012',\n",
       "  '349.570007',\n",
       "  '352.220001',\n",
       "  '352.220001',\n",
       "  '6725100'],\n",
       " ['2019-11-21',\n",
       "  '354.510010',\n",
       "  '360.839996',\n",
       "  '354.000000',\n",
       "  '354.829987',\n",
       "  '354.829987',\n",
       "  '6110000'],\n",
       " ['2019-11-22',\n",
       "  '340.160004',\n",
       "  '341.000000',\n",
       "  '330.000000',\n",
       "  '333.040009',\n",
       "  '333.040009',\n",
       "  '16870600'],\n",
       " ['2019-11-25',\n",
       "  '344.320007',\n",
       "  '344.570007',\n",
       "  '334.459991',\n",
       "  '336.339996',\n",
       "  '336.339996',\n",
       "  '12339500'],\n",
       " ['2019-11-26',\n",
       "  '335.269989',\n",
       "  '335.500000',\n",
       "  '327.100006',\n",
       "  '328.920013',\n",
       "  '328.920013',\n",
       "  '7947400'],\n",
       " ['2019-11-27',\n",
       "  '331.119995',\n",
       "  '333.929993',\n",
       "  '328.570007',\n",
       "  '331.290009',\n",
       "  '331.290009',\n",
       "  '5555600'],\n",
       " ['2019-11-29',\n",
       "  '331.109985',\n",
       "  '331.260010',\n",
       "  '327.500000',\n",
       "  '329.940002',\n",
       "  '329.940002',\n",
       "  '2465600'],\n",
       " ['2019-12-02',\n",
       "  '329.399994',\n",
       "  '336.380005',\n",
       "  '328.690002',\n",
       "  '334.869995',\n",
       "  '334.869995',\n",
       "  '6074500'],\n",
       " ['2019-12-03',\n",
       "  '332.619995',\n",
       "  '337.910004',\n",
       "  '332.190002',\n",
       "  '336.200012',\n",
       "  '336.200012',\n",
       "  '6573700'],\n",
       " ['2019-12-04',\n",
       "  '337.750000',\n",
       "  '337.859985',\n",
       "  '332.850006',\n",
       "  '333.029999',\n",
       "  '333.029999',\n",
       "  '5533000'],\n",
       " ['2019-12-05',\n",
       "  '332.829987',\n",
       "  '334.420013',\n",
       "  '327.250000',\n",
       "  '330.369995',\n",
       "  '330.369995',\n",
       "  '3724600'],\n",
       " ['2019-12-06',\n",
       "  '335.000000',\n",
       "  '338.859985',\n",
       "  '334.769989',\n",
       "  '335.890015',\n",
       "  '335.890015',\n",
       "  '7612400'],\n",
       " ['2019-12-09',\n",
       "  '336.589996',\n",
       "  '344.450012',\n",
       "  '335.079987',\n",
       "  '339.529999',\n",
       "  '339.529999',\n",
       "  '9023100'],\n",
       " ['2019-12-10',\n",
       "  '339.959991',\n",
       "  '350.730011',\n",
       "  '339.309998',\n",
       "  '348.839996',\n",
       "  '348.839996',\n",
       "  '8828300'],\n",
       " ['2019-12-11',\n",
       "  '351.880005',\n",
       "  '357.190002',\n",
       "  '351.089996',\n",
       "  '352.700012',\n",
       "  '352.700012',\n",
       "  '6897800'],\n",
       " ['2019-12-12',\n",
       "  '354.920013',\n",
       "  '362.739990',\n",
       "  '353.230011',\n",
       "  '359.679993',\n",
       "  '359.679993',\n",
       "  '7763900'],\n",
       " ['2019-12-13',\n",
       "  '361.049988',\n",
       "  '365.209991',\n",
       "  '354.640015',\n",
       "  '358.390015',\n",
       "  '358.390015',\n",
       "  '6570900'],\n",
       " ['2019-12-16',\n",
       "  '362.549988',\n",
       "  '383.609985',\n",
       "  '362.500000',\n",
       "  '381.500000',\n",
       "  '381.500000',\n",
       "  '18174200'],\n",
       " ['2019-12-17',\n",
       "  '378.989990',\n",
       "  '385.500000',\n",
       "  '375.899994',\n",
       "  '378.989990',\n",
       "  '378.989990',\n",
       "  '8496800'],\n",
       " ['2019-12-18',\n",
       "  '380.630005',\n",
       "  '395.220001',\n",
       "  '380.579987',\n",
       "  '393.149994',\n",
       "  '393.149994',\n",
       "  '14121000'],\n",
       " ['2019-12-19',\n",
       "  '397.320007',\n",
       "  '406.850006',\n",
       "  '396.500000',\n",
       "  '404.040009',\n",
       "  '404.040009',\n",
       "  '18107100'],\n",
       " ['2019-12-20',\n",
       "  '410.290009',\n",
       "  '413.000000',\n",
       "  '400.190002',\n",
       "  '405.589996',\n",
       "  '405.589996',\n",
       "  '14752700'],\n",
       " ['2019-12-23',\n",
       "  '411.779999',\n",
       "  '422.010010',\n",
       "  '410.000000',\n",
       "  '419.220001',\n",
       "  '419.220001',\n",
       "  '13319600'],\n",
       " ['2019-12-24',\n",
       "  '418.359985',\n",
       "  '425.470001',\n",
       "  '412.690002',\n",
       "  '425.250000',\n",
       "  '425.250000',\n",
       "  '8054700'],\n",
       " ['2019-12-26',\n",
       "  '427.910004',\n",
       "  '433.480011',\n",
       "  '426.350006',\n",
       "  '430.940002',\n",
       "  '430.940002',\n",
       "  '10633900'],\n",
       " ['2019-12-27',\n",
       "  '435.000000',\n",
       "  '435.309998',\n",
       "  '426.109985',\n",
       "  '430.380005',\n",
       "  '430.380005',\n",
       "  '9945700'],\n",
       " ['2019-12-30',\n",
       "  '428.790009',\n",
       "  '429.000000',\n",
       "  '409.260010',\n",
       "  '414.700012',\n",
       "  '414.700012',\n",
       "  '12586400'],\n",
       " ['2019-12-31',\n",
       "  '405.000000',\n",
       "  '421.290009',\n",
       "  '402.079987',\n",
       "  '418.329987',\n",
       "  '418.329987',\n",
       "  '10285700'],\n",
       " ['2020-01-02',\n",
       "  '424.500000',\n",
       "  '430.700012',\n",
       "  '421.709991',\n",
       "  '430.260010',\n",
       "  '430.260010',\n",
       "  '9532100'],\n",
       " ['2020-01-03',\n",
       "  '440.500000',\n",
       "  '454.000000',\n",
       "  '436.920013',\n",
       "  '443.010010',\n",
       "  '443.010010',\n",
       "  '17778500'],\n",
       " ['2020-01-06',\n",
       "  '440.470001',\n",
       "  '451.559998',\n",
       "  '440.000000',\n",
       "  '451.540009',\n",
       "  '451.540009',\n",
       "  '10133000'],\n",
       " ['2020-01-07',\n",
       "  '461.399994',\n",
       "  '471.630005',\n",
       "  '453.359985',\n",
       "  '469.059998',\n",
       "  '469.059998',\n",
       "  '17882100'],\n",
       " ['2020-01-08',\n",
       "  '473.700012',\n",
       "  '498.489990',\n",
       "  '468.230011',\n",
       "  '492.140015',\n",
       "  '492.140015',\n",
       "  '31144300'],\n",
       " ['2020-01-09',\n",
       "  '497.100006',\n",
       "  '498.799988',\n",
       "  '472.869995',\n",
       "  '481.339996',\n",
       "  '481.339996',\n",
       "  '28440400'],\n",
       " ['2020-01-10',\n",
       "  '481.790009',\n",
       "  '484.940002',\n",
       "  '473.700012',\n",
       "  '478.149994',\n",
       "  '478.149994',\n",
       "  '12959500'],\n",
       " ['2020-01-13',\n",
       "  '493.500000',\n",
       "  '525.630005',\n",
       "  '492.000000',\n",
       "  '524.859985',\n",
       "  '524.859985',\n",
       "  '26517600'],\n",
       " ['2020-01-14',\n",
       "  '544.260010',\n",
       "  '547.409973',\n",
       "  '524.900024',\n",
       "  '537.919983',\n",
       "  '537.919983',\n",
       "  '28996200'],\n",
       " ['2020-01-15',\n",
       "  '529.760010',\n",
       "  '537.840027',\n",
       "  '516.789978',\n",
       "  '518.500000',\n",
       "  '518.500000',\n",
       "  '17368800'],\n",
       " ['2020-01-16',\n",
       "  '493.750000',\n",
       "  '514.460022',\n",
       "  '492.170013',\n",
       "  '513.489990',\n",
       "  '513.489990',\n",
       "  '21736700'],\n",
       " ['2020-01-17',\n",
       "  '507.609985',\n",
       "  '515.669983',\n",
       "  '503.160004',\n",
       "  '510.500000',\n",
       "  '510.500000',\n",
       "  '13629100'],\n",
       " ['2020-01-21',\n",
       "  '530.250000',\n",
       "  '548.580017',\n",
       "  '528.409973',\n",
       "  '547.200012',\n",
       "  '547.200012',\n",
       "  '17803500'],\n",
       " ['2020-01-22',\n",
       "  '571.890015',\n",
       "  '594.500000',\n",
       "  '559.099976',\n",
       "  '569.559998',\n",
       "  '569.559998',\n",
       "  '31369000'],\n",
       " ['2020-01-23',\n",
       "  '564.250000',\n",
       "  '582.000000',\n",
       "  '555.599976',\n",
       "  '572.200012',\n",
       "  '572.200012',\n",
       "  '19651000'],\n",
       " ['2020-01-24',\n",
       "  '570.630005',\n",
       "  '573.859985',\n",
       "  '554.260010',\n",
       "  '564.820007',\n",
       "  '564.820007',\n",
       "  '14353600'],\n",
       " ['2020-01-27',\n",
       "  '541.989990',\n",
       "  '564.440002',\n",
       "  '539.280029',\n",
       "  '558.020020',\n",
       "  '558.020020',\n",
       "  '13608100'],\n",
       " ['2020-01-28',\n",
       "  '568.489990',\n",
       "  '576.809998',\n",
       "  '558.080017',\n",
       "  '566.900024',\n",
       "  '566.900024',\n",
       "  '11788500'],\n",
       " ['2020-01-29',\n",
       "  '575.690002',\n",
       "  '589.799988',\n",
       "  '567.429993',\n",
       "  '580.989990',\n",
       "  '580.989990',\n",
       "  '17801500'],\n",
       " ['2020-01-30',\n",
       "  '632.419983',\n",
       "  '650.880005',\n",
       "  '618.000000',\n",
       "  '640.809998',\n",
       "  '640.809998',\n",
       "  '29005700'],\n",
       " ['2020-01-31',\n",
       "  '640.000000',\n",
       "  '653.000000',\n",
       "  '632.520020',\n",
       "  '650.570007',\n",
       "  '650.570007',\n",
       "  '15719300'],\n",
       " ['2020-02-03',\n",
       "  '673.690002',\n",
       "  '786.140015',\n",
       "  '673.520020',\n",
       "  '780.000000',\n",
       "  '780.000000',\n",
       "  '47233500'],\n",
       " ['2020-02-04',\n",
       "  '882.960022',\n",
       "  '968.989990',\n",
       "  '833.880005',\n",
       "  '887.059998',\n",
       "  '887.059998',\n",
       "  '60938800'],\n",
       " ['2020-02-05',\n",
       "  '823.260010',\n",
       "  '845.979980',\n",
       "  '704.109985',\n",
       "  '734.700012',\n",
       "  '734.700012',\n",
       "  '48423800'],\n",
       " ['2020-02-06',\n",
       "  '699.919983',\n",
       "  '795.830017',\n",
       "  '687.000000',\n",
       "  '748.960022',\n",
       "  '748.960022',\n",
       "  '39880800'],\n",
       " ['2020-02-07',\n",
       "  '730.549988',\n",
       "  '769.750000',\n",
       "  '730.000000',\n",
       "  '748.070007',\n",
       "  '748.070007',\n",
       "  '17063500'],\n",
       " ['2020-02-10',\n",
       "  '800.000000',\n",
       "  '819.989990',\n",
       "  '752.400024',\n",
       "  '771.280029',\n",
       "  '771.280029',\n",
       "  '24689200'],\n",
       " ['2020-02-11',\n",
       "  '768.789978',\n",
       "  '783.510010',\n",
       "  '758.000000',\n",
       "  '774.380005',\n",
       "  '774.380005',\n",
       "  '11697500'],\n",
       " ['2020-02-12',\n",
       "  '777.869995',\n",
       "  '789.750000',\n",
       "  '763.369995',\n",
       "  '767.289978',\n",
       "  '767.289978',\n",
       "  '12022500'],\n",
       " ['2020-02-13',\n",
       "  '741.840027',\n",
       "  '818.000000',\n",
       "  '735.000000',\n",
       "  '804.000000',\n",
       "  '804.000000',\n",
       "  '26289300'],\n",
       " ['2020-02-14',\n",
       "  '787.219971',\n",
       "  '812.969971',\n",
       "  '785.500000',\n",
       "  '800.030029',\n",
       "  '800.030029',\n",
       "  '15693700'],\n",
       " ['2020-02-18',\n",
       "  '841.599976',\n",
       "  '860.000000',\n",
       "  '832.359985',\n",
       "  '858.400024',\n",
       "  '858.400024',\n",
       "  '16381700'],\n",
       " ['2020-02-19',\n",
       "  '923.500000',\n",
       "  '944.780029',\n",
       "  '901.020020',\n",
       "  '917.419983',\n",
       "  '917.419983',\n",
       "  '25423000'],\n",
       " ['2020-02-20',\n",
       "  '911.950012',\n",
       "  '912.000000',\n",
       "  '859.940002',\n",
       "  '899.409973',\n",
       "  '899.409973',\n",
       "  '17634900'],\n",
       " ['2020-02-21',\n",
       "  '906.979980',\n",
       "  '913.059998',\n",
       "  '880.450012',\n",
       "  '901.000000',\n",
       "  '901.000000',\n",
       "  '14314800'],\n",
       " ['2020-02-24',\n",
       "  '839.000000',\n",
       "  '863.500000',\n",
       "  '822.200012',\n",
       "  '833.789978',\n",
       "  '833.789978',\n",
       "  '15192200'],\n",
       " ['2020-02-25',\n",
       "  '849.000000',\n",
       "  '856.599976',\n",
       "  '787.000000',\n",
       "  '799.909973',\n",
       "  '799.909973',\n",
       "  '17290500'],\n",
       " ['2020-02-26',\n",
       "  '782.500000',\n",
       "  '813.309998',\n",
       "  '776.109985',\n",
       "  '778.799988',\n",
       "  '778.799988',\n",
       "  '14085500'],\n",
       " ['2020-02-27',\n",
       "  '730.000000',\n",
       "  '739.770020',\n",
       "  '669.000000',\n",
       "  '679.000000',\n",
       "  '679.000000',\n",
       "  '24149300'],\n",
       " ['2020-02-28',\n",
       "  '629.700012',\n",
       "  '690.520020',\n",
       "  '611.520020',\n",
       "  '667.989990',\n",
       "  '667.989990',\n",
       "  '24564200'],\n",
       " ['2020-03-02',\n",
       "  '711.260010',\n",
       "  '743.690002',\n",
       "  '686.669983',\n",
       "  '743.619995',\n",
       "  '743.619995',\n",
       "  '20195000'],\n",
       " ['2020-03-03',\n",
       "  '805.000000',\n",
       "  '806.979980',\n",
       "  '716.109985',\n",
       "  '745.510010',\n",
       "  '745.510010',\n",
       "  '25784000'],\n",
       " ['2020-03-04',\n",
       "  '763.960022',\n",
       "  '766.520020',\n",
       "  '724.729980',\n",
       "  '749.500000',\n",
       "  '749.500000',\n",
       "  '15049000'],\n",
       " ['2020-03-05',\n",
       "  '723.770020',\n",
       "  '745.750000',\n",
       "  '718.070007',\n",
       "  '724.539978',\n",
       "  '724.539978',\n",
       "  '10852700'],\n",
       " ['2020-03-06',\n",
       "  '690.000000',\n",
       "  '707.000000',\n",
       "  '684.270020',\n",
       "  '703.479980',\n",
       "  '703.479980',\n",
       "  '12662900'],\n",
       " ['2020-03-09',\n",
       "  '605.390015',\n",
       "  '663.000000',\n",
       "  '605.000000',\n",
       "  '608.000000',\n",
       "  '608.000000',\n",
       "  '17073700'],\n",
       " ['2020-03-10',\n",
       "  '659.429993',\n",
       "  '668.000000',\n",
       "  '608.000000',\n",
       "  '645.330017',\n",
       "  '645.330017',\n",
       "  '15594400'],\n",
       " ['2020-03-11',\n",
       "  '640.200012',\n",
       "  '653.580017',\n",
       "  '613.000000',\n",
       "  '634.229980',\n",
       "  '634.229980',\n",
       "  '13322500'],\n",
       " ['2020-03-12',\n",
       "  '580.890015',\n",
       "  '594.500000',\n",
       "  '546.250000',\n",
       "  '560.549988',\n",
       "  '560.549988',\n",
       "  '18909100'],\n",
       " ['2020-03-13',\n",
       "  '595.000000',\n",
       "  '607.570007',\n",
       "  '502.000000',\n",
       "  '546.619995',\n",
       "  '546.619995',\n",
       "  '22640300'],\n",
       " ['2020-03-16',\n",
       "  '469.500000',\n",
       "  '494.869995',\n",
       "  '442.170013',\n",
       "  '445.070007',\n",
       "  '445.070007',\n",
       "  '20489500'],\n",
       " ['2020-03-17',\n",
       "  '440.010010',\n",
       "  '471.850006',\n",
       "  '396.000000',\n",
       "  '430.200012',\n",
       "  '430.200012',\n",
       "  '23994600'],\n",
       " ['2020-03-18',\n",
       "  '389.000000',\n",
       "  '404.859985',\n",
       "  '350.510010',\n",
       "  '361.220001',\n",
       "  '361.220001',\n",
       "  '23786200'],\n",
       " ['2020-03-19',\n",
       "  '374.700012',\n",
       "  '452.000000',\n",
       "  '358.459991',\n",
       "  '427.640015',\n",
       "  '427.640015',\n",
       "  '30195500'],\n",
       " ['2020-03-20',\n",
       "  '438.200012',\n",
       "  '477.000000',\n",
       "  '425.790009',\n",
       "  '427.529999',\n",
       "  '427.529999',\n",
       "  '28285500'],\n",
       " ['2020-03-23',\n",
       "  '433.600006',\n",
       "  '442.000000',\n",
       "  '410.500000',\n",
       "  '434.290009',\n",
       "  '434.290009',\n",
       "  '16454500'],\n",
       " ['2020-03-24',\n",
       "  '477.299988',\n",
       "  '513.690002',\n",
       "  '474.000000',\n",
       "  '505.000000',\n",
       "  '505.000000',\n",
       "  '22895200'],\n",
       " ['2020-03-25',\n",
       "  '545.250000',\n",
       "  '557.000000',\n",
       "  '511.109985',\n",
       "  '539.250000',\n",
       "  '539.250000',\n",
       "  '21222700'],\n",
       " ['2020-03-26',\n",
       "  '547.390015',\n",
       "  '560.000000',\n",
       "  '512.250000',\n",
       "  '528.159973',\n",
       "  '528.159973',\n",
       "  '17380700'],\n",
       " ['2020-03-27',\n",
       "  '505.000000',\n",
       "  '525.799988',\n",
       "  '494.029999',\n",
       "  '514.359985',\n",
       "  '514.359985',\n",
       "  '14377400'],\n",
       " ['2020-03-30',\n",
       "  '510.260010',\n",
       "  '516.650024',\n",
       "  '491.230011',\n",
       "  '502.130005',\n",
       "  '502.130005',\n",
       "  '11998100'],\n",
       " ['2020-03-31',\n",
       "  '501.250000',\n",
       "  '542.960022',\n",
       "  '497.000000',\n",
       "  '524.000000',\n",
       "  '524.000000',\n",
       "  '17771500'],\n",
       " ['2020-04-01',\n",
       "  '504.000000',\n",
       "  '513.950012',\n",
       "  '475.100006',\n",
       "  '481.559998',\n",
       "  '481.559998',\n",
       "  '13353200'],\n",
       " ['2020-04-02',\n",
       "  '481.029999',\n",
       "  '494.260010',\n",
       "  '446.399994',\n",
       "  '454.470001',\n",
       "  '454.470001',\n",
       "  '19858400'],\n",
       " ['2020-04-03',\n",
       "  '509.500000',\n",
       "  '515.489990',\n",
       "  '468.390015',\n",
       "  '480.010010',\n",
       "  '480.010010',\n",
       "  '22562100'],\n",
       " ['2020-04-06',\n",
       "  '511.200012',\n",
       "  '521.000000',\n",
       "  '497.959991',\n",
       "  '516.239990',\n",
       "  '516.239990',\n",
       "  '14901800'],\n",
       " ['2020-04-07',\n",
       "  '545.000000',\n",
       "  '565.000000',\n",
       "  '532.340027',\n",
       "  '545.450012',\n",
       "  '545.450012',\n",
       "  '17919800'],\n",
       " ['2020-04-08',\n",
       "  '554.200012',\n",
       "  '557.210022',\n",
       "  '533.330017',\n",
       "  '548.840027',\n",
       "  '548.840027',\n",
       "  '12656000'],\n",
       " ['2020-04-09',\n",
       "  '562.090027',\n",
       "  '575.179993',\n",
       "  '557.109985',\n",
       "  '573.000000',\n",
       "  '573.000000',\n",
       "  '13650000'],\n",
       " ['2020-04-13',\n",
       "  '590.159973',\n",
       "  '652.000000',\n",
       "  '580.530029',\n",
       "  '650.950012',\n",
       "  '650.950012',\n",
       "  '22475400'],\n",
       " ['2020-04-14',\n",
       "  '698.969971',\n",
       "  '741.880005',\n",
       "  '692.429993',\n",
       "  '709.890015',\n",
       "  '709.890015',\n",
       "  '30576500'],\n",
       " ['2020-04-15',\n",
       "  '742.000000',\n",
       "  '753.130005',\n",
       "  '710.000000',\n",
       "  '729.830017',\n",
       "  '729.830017',\n",
       "  '23577000'],\n",
       " ['2020-04-16',\n",
       "  '716.940002',\n",
       "  '759.450012',\n",
       "  '706.719971',\n",
       "  '745.210022',\n",
       "  '745.210022',\n",
       "  '20657900'],\n",
       " ['2020-04-17',\n",
       "  '772.280029',\n",
       "  '774.950012',\n",
       "  '747.659973',\n",
       "  '753.890015',\n",
       "  '753.890015',\n",
       "  '13128200'],\n",
       " ['2020-04-20',\n",
       "  '732.700012',\n",
       "  '765.570007',\n",
       "  '712.210022',\n",
       "  '746.359985',\n",
       "  '746.359985',\n",
       "  '14746600'],\n",
       " ['2020-04-21',\n",
       "  '730.119995',\n",
       "  '753.330017',\n",
       "  '673.789978',\n",
       "  '686.719971',\n",
       "  '686.719971',\n",
       "  '20209100'],\n",
       " ['2020-04-22',\n",
       "  '703.979980',\n",
       "  '734.000000',\n",
       "  '688.710022',\n",
       "  '732.109985',\n",
       "  '732.109985',\n",
       "  '14224800'],\n",
       " ['2020-04-23',\n",
       "  '727.599976',\n",
       "  '734.000000',\n",
       "  '703.130005',\n",
       "  '705.630005',\n",
       "  '705.630005',\n",
       "  '13236700'],\n",
       " ['2020-04-24',\n",
       "  '710.809998',\n",
       "  '730.729980',\n",
       "  '698.179993',\n",
       "  '725.150024',\n",
       "  '725.150024',\n",
       "  '13237600'],\n",
       " ['2020-04-27',\n",
       "  '737.609985',\n",
       "  '799.489990',\n",
       "  '735.000000',\n",
       "  '798.750000',\n",
       "  '798.750000',\n",
       "  '20681400'],\n",
       " ['2020-04-28',\n",
       "  '795.640015',\n",
       "  '805.000000',\n",
       "  '756.690002',\n",
       "  '769.119995',\n",
       "  '769.119995',\n",
       "  '15222000'],\n",
       " ['2020-04-29',\n",
       "  '790.169983',\n",
       "  '803.200012',\n",
       "  '783.159973',\n",
       "  '800.510010',\n",
       "  '800.510010',\n",
       "  '16216000'],\n",
       " ['2020-04-30',\n",
       "  '855.190002',\n",
       "  '869.820007',\n",
       "  '763.500000',\n",
       "  '781.880005',\n",
       "  '781.880005',\n",
       "  '28400100'],\n",
       " ['2020-05-01',\n",
       "  '755.000000',\n",
       "  '772.770020',\n",
       "  '683.039978',\n",
       "  '701.320007',\n",
       "  '701.320007',\n",
       "  '32531800'],\n",
       " ['2020-05-04',\n",
       "  '701.000000',\n",
       "  '762.000000',\n",
       "  '698.000000',\n",
       "  '761.190002',\n",
       "  '761.190002',\n",
       "  '19237100'],\n",
       " ['2020-05-05',\n",
       "  '789.789978',\n",
       "  '798.919983',\n",
       "  '762.179993',\n",
       "  '768.210022',\n",
       "  '768.210022',\n",
       "  '16991700'],\n",
       " ['2020-05-06',\n",
       "  '776.500000',\n",
       "  '789.799988',\n",
       "  '761.109985',\n",
       "  '782.580017',\n",
       "  '782.580017',\n",
       "  '11123200'],\n",
       " ['2020-05-07',\n",
       "  '777.210022',\n",
       "  '796.400024',\n",
       "  '772.349976',\n",
       "  '780.039978',\n",
       "  '780.039978',\n",
       "  '11527700'],\n",
       " ['2020-05-08',\n",
       "  '793.770020',\n",
       "  '824.000000',\n",
       "  '787.010010',\n",
       "  '819.419983',\n",
       "  '819.419983',\n",
       "  '16130100'],\n",
       " ['2020-05-11',\n",
       "  '790.510010',\n",
       "  '824.000000',\n",
       "  '785.000000',\n",
       "  '811.289978',\n",
       "  '811.289978',\n",
       "  '16471100'],\n",
       " ['2020-05-12',\n",
       "  '827.000000',\n",
       "  '843.289978',\n",
       "  '808.000000',\n",
       "  '809.409973',\n",
       "  '809.409973',\n",
       "  '15906900'],\n",
       " ['2020-05-13',\n",
       "  '820.830017',\n",
       "  '826.000000',\n",
       "  '763.299988',\n",
       "  '790.960022',\n",
       "  '790.960022',\n",
       "  '19065500'],\n",
       " ['2020-05-14',\n",
       "  '780.000000',\n",
       "  '803.359985',\n",
       "  '764.000000',\n",
       "  '803.330017',\n",
       "  '803.330017',\n",
       "  '13682200'],\n",
       " ['2020-05-15',\n",
       "  '790.349976',\n",
       "  '805.049988',\n",
       "  '786.549988',\n",
       "  '799.169983',\n",
       "  '799.169983',\n",
       "  '10518400'],\n",
       " ['2020-05-18',\n",
       "  '827.780029',\n",
       "  '834.719971',\n",
       "  '803.880005',\n",
       "  '813.630005',\n",
       "  '813.630005',\n",
       "  '11698100'],\n",
       " ['2020-05-19',\n",
       "  '815.169983',\n",
       "  '822.070007',\n",
       "  '806.080017',\n",
       "  '808.010010',\n",
       "  '808.010010',\n",
       "  '9636500'],\n",
       " ['2020-05-20',\n",
       "  '820.500000',\n",
       "  '826.000000',\n",
       "  '811.799988',\n",
       "  '815.559998',\n",
       "  '815.559998',\n",
       "  '7309300'],\n",
       " ['2020-05-21',\n",
       "  '816.000000',\n",
       "  '832.500000',\n",
       "  '796.000000',\n",
       "  '827.599976',\n",
       "  '827.599976',\n",
       "  '12254600'],\n",
       " ['2020-05-22',\n",
       "  '822.169983',\n",
       "  '831.780029',\n",
       "  '812.000000',\n",
       "  '816.880005',\n",
       "  '816.880005',\n",
       "  '9987500'],\n",
       " ['2020-05-26',\n",
       "  '834.500000',\n",
       "  '834.599976',\n",
       "  '815.710022',\n",
       "  '818.869995',\n",
       "  '818.869995',\n",
       "  '8089700'],\n",
       " ['2020-05-27',\n",
       "  '820.859985',\n",
       "  '827.710022',\n",
       "  '785.000000',\n",
       "  '820.229980',\n",
       "  '820.229980',\n",
       "  '11549500'],\n",
       " ['2020-05-28',\n",
       "  '813.510010',\n",
       "  '824.750000',\n",
       "  '801.690002',\n",
       "  '805.809998',\n",
       "  '805.809998',\n",
       "  '7255600'],\n",
       " ['2020-05-29',\n",
       "  '808.750000',\n",
       "  '835.000000',\n",
       "  '804.210022',\n",
       "  '835.000000',\n",
       "  '835.000000',\n",
       "  '11812500'],\n",
       " ['2020-06-01',\n",
       "  '858.000000',\n",
       "  '899.000000',\n",
       "  '854.099976',\n",
       "  '898.099976',\n",
       "  '898.099976',\n",
       "  '14939500'],\n",
       " ['2020-06-02',\n",
       "  '894.700012',\n",
       "  '908.659973',\n",
       "  '871.000000',\n",
       "  '881.559998',\n",
       "  '881.559998',\n",
       "  '13565600'],\n",
       " ['2020-06-03',\n",
       "  '888.119995',\n",
       "  '897.940002',\n",
       "  '880.099976',\n",
       "  '882.960022',\n",
       "  '882.960022',\n",
       "  '7949500'],\n",
       " ['2020-06-04',\n",
       "  '889.880005',\n",
       "  '895.750000',\n",
       "  '858.440002',\n",
       "  '864.380005',\n",
       "  '864.380005',\n",
       "  '8887700'],\n",
       " ['2020-06-05',\n",
       "  '877.840027',\n",
       "  '886.520020',\n",
       "  '866.200012',\n",
       "  '885.659973',\n",
       "  '885.659973',\n",
       "  '7811900'],\n",
       " ['2020-06-08',\n",
       "  '919.000000',\n",
       "  '950.000000',\n",
       "  '909.159973',\n",
       "  '949.919983',\n",
       "  '949.919983',\n",
       "  '14174700'],\n",
       " ['2020-06-09',\n",
       "  '940.010010',\n",
       "  '954.440002',\n",
       "  '923.929993',\n",
       "  '940.669983',\n",
       "  '940.669983',\n",
       "  '11388200'],\n",
       " ['2020-06-10',\n",
       "  '991.880005',\n",
       "  '1027.479980',\n",
       "  '982.500000',\n",
       "  '1025.050049',\n",
       "  '1025.050049',\n",
       "  '18563400'],\n",
       " ['2020-06-11',\n",
       "  '990.200012',\n",
       "  '1018.960022',\n",
       "  '972.000000',\n",
       "  '972.840027',\n",
       "  '972.840027',\n",
       "  '15916500'],\n",
       " ['2020-06-12',\n",
       "  '980.000000',\n",
       "  '987.979980',\n",
       "  '912.599976',\n",
       "  '935.280029',\n",
       "  '935.280029',\n",
       "  '16730200'],\n",
       " ['2020-06-15',\n",
       "  '917.789978',\n",
       "  '998.840027',\n",
       "  '908.500000',\n",
       "  '990.900024',\n",
       "  '990.900024',\n",
       "  '15697200'],\n",
       " ['2020-06-16',\n",
       "  '1011.849976',\n",
       "  '1012.880005',\n",
       "  '962.390015',\n",
       "  '982.130005',\n",
       "  '982.130005',\n",
       "  '14051100'],\n",
       " ['2020-06-17',\n",
       "  '987.710022',\n",
       "  '1005.000000',\n",
       "  '982.570007',\n",
       "  '991.789978',\n",
       "  '991.789978',\n",
       "  '9869400'],\n",
       " ['2020-06-18',\n",
       "  '1003.000000',\n",
       "  '1019.200012',\n",
       "  '994.469971',\n",
       "  '1003.960022',\n",
       "  '1003.960022',\n",
       "  '9751900'],\n",
       " ['2020-06-19',\n",
       "  '1012.780029',\n",
       "  '1015.969971',\n",
       "  '991.340027',\n",
       "  '1000.900024',\n",
       "  '1000.900024',\n",
       "  '8679700'],\n",
       " ['2020-06-22',\n",
       "  '999.950012',\n",
       "  '1008.880005',\n",
       "  '990.020020',\n",
       "  '994.320007',\n",
       "  '994.320007',\n",
       "  '6362400'],\n",
       " ['2020-06-23',\n",
       "  '998.880005',\n",
       "  '1012.000000',\n",
       "  '994.010010',\n",
       "  '1001.780029',\n",
       "  '1001.780029',\n",
       "  '6365300'],\n",
       " ['2020-06-24',\n",
       "  '994.109985',\n",
       "  '1000.880005',\n",
       "  '953.140015',\n",
       "  '960.849976',\n",
       "  '960.849976',\n",
       "  '10959600'],\n",
       " ['2020-06-25',\n",
       "  '954.270020',\n",
       "  '985.979980',\n",
       "  '937.150024',\n",
       "  '985.979980',\n",
       "  '985.979980',\n",
       "  '9254500'],\n",
       " ['2020-06-26',\n",
       "  '994.780029',\n",
       "  '995.000000',\n",
       "  '954.869995',\n",
       "  '959.739990',\n",
       "  '959.739990',\n",
       "  '8854900'],\n",
       " ['2020-06-29',\n",
       "  '969.010010',\n",
       "  '1010.000000',\n",
       "  '948.520020',\n",
       "  '1009.349976',\n",
       "  '1009.349976',\n",
       "  '9026400'],\n",
       " ['2020-06-30',\n",
       "  '1006.500000',\n",
       "  '1087.689941',\n",
       "  '1003.729980',\n",
       "  '1079.810059',\n",
       "  '1079.810059',\n",
       "  '16918500'],\n",
       " ['2020-07-01',\n",
       "  '1083.000000',\n",
       "  '1135.329956',\n",
       "  '1080.500000',\n",
       "  '1119.630005',\n",
       "  '1119.630005',\n",
       "  '13326900'],\n",
       " ['2020-07-02',\n",
       "  '1221.479980',\n",
       "  '1228.000000',\n",
       "  '1185.599976',\n",
       "  '1208.660034',\n",
       "  '1208.660034',\n",
       "  '17250100'],\n",
       " ['2020-07-06',\n",
       "  '1276.689941',\n",
       "  '1377.790039',\n",
       "  '1266.040039',\n",
       "  '1371.579956',\n",
       "  '1371.579956',\n",
       "  '20569900'],\n",
       " ['2020-07-07',\n",
       "  '1405.010010',\n",
       "  '1429.500000',\n",
       "  '1336.709961',\n",
       "  '1389.859985',\n",
       "  '1389.859985',\n",
       "  '21489700'],\n",
       " ['2020-07-08',\n",
       "  '1405.000000',\n",
       "  '1417.260010',\n",
       "  '1311.339966',\n",
       "  '1365.880005',\n",
       "  '1365.880005',\n",
       "  '16311300'],\n",
       " ['2020-07-09',\n",
       "  '1396.989990',\n",
       "  '1408.560059',\n",
       "  '1351.280029',\n",
       "  '1394.280029',\n",
       "  '1394.280029',\n",
       "  '11717600'],\n",
       " ['2020-07-10',\n",
       "  '1396.000000',\n",
       "  '1548.920044',\n",
       "  '1376.010010',\n",
       "  '1544.650024',\n",
       "  '1544.650024',\n",
       "  '23337600'],\n",
       " ['2020-07-13',\n",
       "  '1659.000000',\n",
       "  '1794.989990',\n",
       "  '1471.109985',\n",
       "  '1497.060059',\n",
       "  '1497.060059',\n",
       "  '38725700'],\n",
       " ['2020-07-14',\n",
       "  '1556.000000',\n",
       "  '1590.000000',\n",
       "  '1431.000000',\n",
       "  '1516.800049',\n",
       "  '1516.800049',\n",
       "  '22833862']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_file = 'AMZN.csv'\n",
    "amazon_rdd = sc.textFile(amazon_file)\n",
    "csv_rdd2 = amazon_rdd.map(lambda row:row.split(\",\"))\n",
    "\n",
    "google_file = 'GOOG.csv'\n",
    "google_rdd = sc.textFile(google_file)\n",
    "csv_rdd3 = google_rdd.map(lambda row:row.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = [('Alice', 1)]\n",
    "df = sqlContext.createDataFrame(li, ['name', 'age'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='Date', open='Open', high='High', low='Low', close='Close', adjclose='AdjClose', volume='Volume'),\n",
       " Row(date='2019-07-15', open='248.000000', high='254.419998', low='244.860001', close='253.500000', adjclose='253.500000', volume='11000100'),\n",
       " Row(date='2019-07-16', open='249.300003', high='253.529999', low='247.929993', close='252.380005', adjclose='252.380005', volume='8149000'),\n",
       " Row(date='2019-07-17', open='255.669998', high='258.309998', low='253.350006', close='254.860001', adjclose='254.860001', volume='9764700'),\n",
       " Row(date='2019-07-18', open='255.050003', high='255.750000', low='251.889999', close='253.539993', adjclose='253.539993', volume='4764500')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla_df = csv_rdd.toDF(['date', 'open','high', 'low', 'close', 'adjclose', 'volume'])\n",
    "tesla_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df = sqlContext.read.load(amazon_file,\n",
    "                                format='com.databricks.spark.csv',\n",
    "                                header='true',\n",
    "                                inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2019-07-15', Open=2021.400024, High=2022.900024, Low=2001.550049, Close=2020.98999, AdjClose=2020.98999, Volume=2981300),\n",
       " Row(Date='2019-07-16', Open=2010.579956, High=2026.319946, Low=2001.219971, Close=2009.900024, AdjClose=2009.900024, Volume=2618200)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- AdjClose: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|      Date|       Open|       High|        Low|      Close|   AdjClose| Volume|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|2019-07-15|2021.400024|2022.900024|2001.550049| 2020.98999| 2020.98999|2981300|\n",
      "|2019-07-16|2010.579956|2026.319946|2001.219971|2009.900024|2009.900024|2618200|\n",
      "|2019-07-17|2007.050049|     2012.0|1992.030029|1992.030029|1992.030029|2558800|\n",
      "|2019-07-18| 1980.01001|     1987.5|1951.550049|1977.900024|1977.900024|3504300|\n",
      "|2019-07-19|1991.209961|     1996.0| 1962.22998| 1964.52002| 1964.52002|3185600|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>2021.400024</td>\n",
       "      <td>2022.900024</td>\n",
       "      <td>2001.550049</td>\n",
       "      <td>2020.989990</td>\n",
       "      <td>2020.989990</td>\n",
       "      <td>2981300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>2010.579956</td>\n",
       "      <td>2026.319946</td>\n",
       "      <td>2001.219971</td>\n",
       "      <td>2009.900024</td>\n",
       "      <td>2009.900024</td>\n",
       "      <td>2618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-17</td>\n",
       "      <td>2007.050049</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>1992.030029</td>\n",
       "      <td>1992.030029</td>\n",
       "      <td>1992.030029</td>\n",
       "      <td>2558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>1980.010010</td>\n",
       "      <td>1987.500000</td>\n",
       "      <td>1951.550049</td>\n",
       "      <td>1977.900024</td>\n",
       "      <td>1977.900024</td>\n",
       "      <td>3504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>1991.209961</td>\n",
       "      <td>1996.000000</td>\n",
       "      <td>1962.229980</td>\n",
       "      <td>1964.520020</td>\n",
       "      <td>1964.520020</td>\n",
       "      <td>3185600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2019-07-15  2021.400024  2022.900024  2001.550049  2020.989990   \n",
       "1  2019-07-16  2010.579956  2026.319946  2001.219971  2009.900024   \n",
       "2  2019-07-17  2007.050049  2012.000000  1992.030029  1992.030029   \n",
       "3  2019-07-18  1980.010010  1987.500000  1951.550049  1977.900024   \n",
       "4  2019-07-19  1991.209961  1996.000000  1962.229980  1964.520020   \n",
       "\n",
       "      AdjClose   Volume  \n",
       "0  2020.989990  2981300  \n",
       "1  2009.900024  2618200  \n",
       "2  1992.030029  2558800  \n",
       "3  1977.900024  3504300  \n",
       "4  1964.520020  3185600  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "amazon_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df = sqlContext.read.load(google_file,\n",
    "                                format='com.databricks.spark.csv',\n",
    "                                header='true',\n",
    "                                inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2019-07-15', Open=1146.859985, High=1150.819946, Low=1139.400024, Close=1150.339966, AdjClose=1150.339966, Volume=903800),\n",
       " Row(Date='2019-07-16', Open=1146.0, High=1158.579956, Low=1145.0, Close=1153.579956, AdjClose=1153.579956, Volume=1238800)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>1146.859985</td>\n",
       "      <td>1150.819946</td>\n",
       "      <td>1139.400024</td>\n",
       "      <td>1150.339966</td>\n",
       "      <td>1150.339966</td>\n",
       "      <td>903800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>1146.000000</td>\n",
       "      <td>1158.579956</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>1153.579956</td>\n",
       "      <td>1153.579956</td>\n",
       "      <td>1238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-17</td>\n",
       "      <td>1150.969971</td>\n",
       "      <td>1158.359985</td>\n",
       "      <td>1145.770020</td>\n",
       "      <td>1146.349976</td>\n",
       "      <td>1146.349976</td>\n",
       "      <td>1170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>1141.739990</td>\n",
       "      <td>1147.604980</td>\n",
       "      <td>1132.729980</td>\n",
       "      <td>1146.329956</td>\n",
       "      <td>1146.329956</td>\n",
       "      <td>1291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>1148.189941</td>\n",
       "      <td>1151.140015</td>\n",
       "      <td>1129.619995</td>\n",
       "      <td>1130.099976</td>\n",
       "      <td>1130.099976</td>\n",
       "      <td>1647200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2019-07-15  1146.859985  1150.819946  1139.400024  1150.339966   \n",
       "1  2019-07-16  1146.000000  1158.579956  1145.000000  1153.579956   \n",
       "2  2019-07-17  1150.969971  1158.359985  1145.770020  1146.349976   \n",
       "3  2019-07-18  1141.739990  1147.604980  1132.729980  1146.329956   \n",
       "4  2019-07-19  1148.189941  1151.140015  1129.619995  1130.099976   \n",
       "\n",
       "      AdjClose   Volume  \n",
       "0  1150.339966   903800  \n",
       "1  1153.579956  1238800  \n",
       "2  1146.349976  1170000  \n",
       "3  1146.329956  1291300  \n",
       "4  1130.099976  1647200  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  yr|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2019|1245.3833654621849|\n",
      "|2020|1362.8286906865671|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "google_df.select(year(\"Date\").alias('yr'), \"Close\").groupby('yr').avg('Close').sort('yr').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['year ASC NULLS FIRST, 'month ASC NULLS FIRST], true\n",
      "+- Aggregate [year#288, month#289], [year#288, month#289, avg(Low#51) AS avg(Low)#297]\n",
      "   +- Project [year(cast(Date#48 as date)) AS year#288, month(cast(Date#48 as date)) AS month#289, Low#51]\n",
      "      +- Relation[Date#48,Open#49,High#50,Low#51,Close#52,AdjClose#53,Volume#54] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "year: int, month: int, avg(Low): double\n",
      "Sort [year#288 ASC NULLS FIRST, month#289 ASC NULLS FIRST], true\n",
      "+- Aggregate [year#288, month#289], [year#288, month#289, avg(Low#51) AS avg(Low)#297]\n",
      "   +- Project [year(cast(Date#48 as date)) AS year#288, month(cast(Date#48 as date)) AS month#289, Low#51]\n",
      "      +- Relation[Date#48,Open#49,High#50,Low#51,Close#52,AdjClose#53,Volume#54] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [year#288 ASC NULLS FIRST, month#289 ASC NULLS FIRST], true\n",
      "+- Aggregate [year#288, month#289], [year#288, month#289, avg(Low#51) AS avg(Low)#297]\n",
      "   +- Project [year(cast(Date#48 as date)) AS year#288, month(cast(Date#48 as date)) AS month#289, Low#51]\n",
      "      +- Relation[Date#48,Open#49,High#50,Low#51,Close#52,AdjClose#53,Volume#54] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [year#288 ASC NULLS FIRST, month#289 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(year#288 ASC NULLS FIRST, month#289 ASC NULLS FIRST, 200), true, [id=#229]\n",
      "   +- *(2) HashAggregate(keys=[year#288, month#289], functions=[avg(Low#51)], output=[year#288, month#289, avg(Low)#297])\n",
      "      +- Exchange hashpartitioning(year#288, month#289, 200), true, [id=#225]\n",
      "         +- *(1) HashAggregate(keys=[year#288, month#289], functions=[partial_avg(Low#51)], output=[year#288, month#289, sum#303, count#304L])\n",
      "            +- *(1) Project [year(cast(Date#48 as date)) AS year#288, month(cast(Date#48 as date)) AS month#289, Low#51]\n",
      "               +- FileScan csv [Date#48,Low#51] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/AMZN.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Date:string,Low:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.select(year(\"Date\").alias(\"year\"),\n",
    "                month(\"Date\").alias(\"month\"),\n",
    "                \"Low\") \\\n",
    ".groupby(\"year\", \"month\") \\\n",
    ".avg(\"Low\") \\\n",
    ".sort(\"year\",\"month\") \\\n",
    ".explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.registerTempTable(\"amazon_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df.registerTempTable(\"google_stocks\")\n",
    "tesla_df.registerTempTable(\"tesla_stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|      Date|       Open|       High|        Low|      Close|   AdjClose| Volume|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|2019-07-15|1146.859985|1150.819946|1139.400024|1150.339966|1150.339966| 903800|\n",
      "|2019-07-16|     1146.0|1158.579956|     1145.0|1153.579956|1153.579956|1238800|\n",
      "|2019-07-17|1150.969971|1158.359985| 1145.77002|1146.349976|1146.349976|1170000|\n",
      "|2019-07-18| 1141.73999| 1147.60498| 1132.72998|1146.329956|1146.329956|1291300|\n",
      "|2019-07-19|1148.189941|1151.140015|1129.619995|1130.099976|1130.099976|1647200|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM google_stocks\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|year|month|        avg(Close)|\n",
      "+----+-----+------------------+\n",
      "|2019|   10|1752.3317498695653|\n",
      "|2020|    6|      2613.5454545|\n",
      "|2020|    3|1872.3104358636365|\n",
      "|2019|    8|1793.6027220909093|\n",
      "|2020|    4|2228.7052408571426|\n",
      "|2020|    1|1884.2376128571425|\n",
      "|2019|    9|     1799.12099615|\n",
      "|2019|   12|1785.7728446190476|\n",
      "|2020|    7| 3053.100016222222|\n",
      "|2020|    2|2066.1752672631574|\n",
      "|2019|    7|1964.6846265384618|\n",
      "|2019|   11|      1774.2939941|\n",
      "|2020|    5|2394.1840209499996|\n",
      "+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select year(Date) as year, month(Date) as month, avg(Close) from amazon_stocks group by year, month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+------------------+\n",
      "|      Date|       Open|      Close|               dif|\n",
      "+----------+-----------+-----------+------------------+\n",
      "|2019-07-16|     1146.0|1153.579956| 7.579956000000038|\n",
      "|2019-07-17|1150.969971|1146.349976| 4.619995000000017|\n",
      "|2019-07-18| 1141.73999|1146.329956| 4.589966000000004|\n",
      "|2019-07-19|1148.189941|1130.099976| 18.08996500000012|\n",
      "|2019-07-22|1133.449951|1138.069946| 4.619995000000017|\n",
      "|2019-07-24|1131.900024|1137.810059|  5.91003499999988|\n",
      "|2019-07-25|1137.819946|1132.119995|5.6999510000000555|\n",
      "|2019-07-26|1224.040039|1250.410034|26.369995000000017|\n",
      "|2019-07-31|     1223.0|1216.680054| 6.319946000000073|\n",
      "|2019-08-01|1214.030029| 1209.01001|5.0200190000000475|\n",
      "|2019-08-02| 1200.73999| 1193.98999|              6.75|\n",
      "|2019-08-05|1170.040039|1152.319946|17.720092999999906|\n",
      "|2019-08-06|1163.310059|1169.949951| 6.639892000000145|\n",
      "|2019-08-07|     1156.0| 1173.98999|17.989990000000034|\n",
      "|2019-08-08|1182.829956|1204.800049|21.970092999999906|\n",
      "|2019-08-09| 1197.98999| 1188.01001| 9.979980000000069|\n",
      "|2019-08-12|1179.209961|1174.709961|               4.5|\n",
      "|2019-08-13|1171.459961| 1197.27002| 25.81005899999991|\n",
      "|2019-08-14|1176.310059|1164.290039|12.020019999999931|\n",
      "|2019-08-19|1190.089966|1198.449951| 8.359985000000052|\n",
      "+----------+-----------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select Date, Open, Close, abs(Close-Open) as dif from google_stocks where abs(close-open)>4\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-------------+\n",
      "|year|max(AdjClose)|min(AdjClose)|\n",
      "+----+-------------+-------------+\n",
      "|null|     AdjClose|     AdjClose|\n",
      "|2019|   430.940002|   211.399994|\n",
      "|2020|   994.320007|  1000.900024|\n",
      "+----+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select year(Date) as year, \\\n",
    "               max(AdjClose), min(AdjClose) \\\n",
    "               from tesla_stocks \\\n",
    "               group by year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinclose = sqlContext.sql(\"SELECT tesla_stocks.Date,  \\\n",
    "tesla_stocks.Close as teslaclose, amazon_stocks.Close as amazonclose, \\\n",
    "google_stocks.Close as googleclose \\\n",
    "FROM tesla_stocks \\\n",
    "JOIN google_stocks ON tesla_stocks.Date = google_stocks.Date \\\n",
    "JOIN amazon_stocks ON amazon_stocks.Date = tesla_stocks.Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------+\n",
      "|      Date|teslaclose|amazonclose|googleclose|\n",
      "+----------+----------+-----------+-----------+\n",
      "|2019-07-15|253.500000| 2020.98999|1150.339966|\n",
      "|2019-07-16|252.380005|2009.900024|1153.579956|\n",
      "|2019-07-17|254.860001|1992.030029|1146.349976|\n",
      "|2019-07-18|253.539993|1977.900024|1146.329956|\n",
      "|2019-07-19|258.179993| 1964.52002|1130.099976|\n",
      "|2019-07-22|255.679993|1985.630005|1138.069946|\n",
      "|2019-07-23|260.170013| 1994.48999|1146.209961|\n",
      "|2019-07-24|264.880005|2000.810059|1137.810059|\n",
      "|2019-07-25|228.820007|1973.819946|1132.119995|\n",
      "|2019-07-26|228.039993|1943.050049|1250.410034|\n",
      "|2019-07-29|235.770004|1912.449951|1239.410034|\n",
      "|2019-07-30|242.259995|1898.530029|1225.140015|\n",
      "|2019-07-31|241.610001|1866.780029|1216.680054|\n",
      "|2019-08-01|233.850006|1855.319946| 1209.01001|\n",
      "|2019-08-02|234.339996| 1823.23999| 1193.98999|\n",
      "|2019-08-05|228.320007|1765.130005|1152.319946|\n",
      "|2019-08-06|230.750000|1787.829956|1169.949951|\n",
      "|2019-08-07|233.419998|1793.400024| 1173.98999|\n",
      "|2019-08-08|238.300003|1832.890015|1204.800049|\n",
      "|2019-08-09|235.009995|1807.579956| 1188.01001|\n",
      "+----------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinclose.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: joinclose; line 1 pos 166;\n'Sort ['year('joinclose.Date) ASC NULLS FIRST], true\n+- 'Aggregate ['year('joinclose.Date)], ['year('joinclose.Date) AS yr#591, 'avg('joinclose.teslaclose) AS teslaclose#592, 'avg('joinclose.amazonclose) AS amazonclose#593, 'avg('joinclose.googleclose) AS googleclose#594]\n   +- 'UnresolvedRelation [joinclose]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-484ef5549470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT year(joinclose.Date) as yr, avg(joinclose.teslaclose) as teslaclose, avg(joinclose.amazonclose) as amazonclose, avg(joinclose.googleclose) as googleclose from joinclose group By year(joinclose.Date) order by year(joinclose.Date)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \"\"\"\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \"\"\"\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: joinclose; line 1 pos 166;\n'Sort ['year('joinclose.Date) ASC NULLS FIRST], true\n+- 'Aggregate ['year('joinclose.Date)], ['year('joinclose.Date) AS yr#591, 'avg('joinclose.teslaclose) AS teslaclose#592, 'avg('joinclose.amazonclose) AS amazonclose#593, 'avg('joinclose.googleclose) AS googleclose#594]\n   +- 'UnresolvedRelation [joinclose]\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT year(joinclose.Date) as yr, avg(joinclose.teslaclose) as teslaclose, avg(joinclose.amazonclose) as amazonclose, avg(joinclose.googleclose) as googleclose from joinclose group By year(joinclose.Date) order by year(joinclose.Date)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinclose.write.format(\"parquet\").save(\"joinStock.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = sqlContext.read.parquet(\"joinStock.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------+\n",
      "|      Date|teslaclose|amazonclose|googleclose|\n",
      "+----------+----------+-----------+-----------+\n",
      "|2020-01-15|518.500000| 1862.02002|1439.199951|\n",
      "|2020-01-16|513.489990|1877.939941|1451.699951|\n",
      "|2020-01-17|510.500000|1864.719971|1480.390015|\n",
      "|2020-01-21|547.200012|     1892.0|1484.400024|\n",
      "|2020-01-22|569.559998|1887.459961|1485.949951|\n",
      "|2020-01-23|572.200012|1884.579956|1486.650024|\n",
      "|2020-01-24|564.820007|1861.640015|1466.709961|\n",
      "|2020-01-27|558.020020|1828.339966|1433.900024|\n",
      "|2020-01-28|566.900024|    1853.25|1452.560059|\n",
      "|2020-01-29|580.989990|     1858.0|1458.630005|\n",
      "|2020-01-30|640.809998|1870.680054|1455.839966|\n",
      "|2020-01-31|650.570007|2008.719971| 1434.22998|\n",
      "|2020-02-03|780.000000|2004.199951|1485.939941|\n",
      "|2020-02-04|887.059998|2049.669922|1447.069946|\n",
      "|2020-02-05|734.700012|2039.869995| 1448.22998|\n",
      "|2020-02-06|748.960022| 2050.22998| 1476.22998|\n",
      "|2020-02-07|748.070007|2079.280029| 1479.22998|\n",
      "|2020-02-10|771.280029|2133.909912|1508.680054|\n",
      "|2020-02-11|774.380005|2150.800049|1508.790039|\n",
      "|2020-02-12|767.289978|     2160.0| 1518.27002|\n",
      "+----------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- teslaclose: string (nullable = true)\n",
      " |-- amazonclose: double (nullable = true)\n",
      " |-- googleclose: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
